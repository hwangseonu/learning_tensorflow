{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tensorflow로 단순한 선형회귀모델을 만들어보자\n",
    "\n",
    "데이터는 1부터 10까지의 정수형 데이터와 그 데이터들에 2를 곱하고 1을 더한 데이터로 구성한다.\n",
    "\n",
    "잘 학습된다면 가중치는 2, 편향은 1에 가까워질 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "데이터를 준비한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.arange(1, 11, dtype=np.float32)\n",
    "y_data = x_data * 2 + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "가중치와 편향을 초기화한다.\n",
    "\n",
    "`tf.uniform`은 균등확률분포 값을 생성해주는 함수이다.\n",
    "\n",
    "이번에는 -1.0과 1.0 사이의 균등확률분포값으로 초기화한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random.uniform([1], -1.0, 1.0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습에 사용될 연산을 `tf.function`으로 정의한다.\n",
    "\n",
    "데이터와 가중치를 곱하고 편향을 더한다.\n",
    "\n",
    "수식으로는 `W * X + b`가 된다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def hypothesis(x):\n",
    "    return tf.add(tf.multiply(W, x), b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습에 사용할 optimizer를 정한다.\n",
    "\n",
    "이번에는 확률적 경사하강법 (Stochastic Gradient Descent)를 사용한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.SGD(learning_rate=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습 함수를 `tf.function`으로 정의한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = hypothesis(x_data)\n",
    "        loss = tf.reduce_mean(tf.square(predictions - y_data))\n",
    "    gradients = tape.gradient(loss, [W, b])\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "100번의 학습을 진행한다. `tf.GradientTape`를 이용하여 gradient를 계산하고 weight를 업데이트한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. 356.55499267578125 [1.347764] [1.0024102]\n",
      "1. 16.361068725585938 [1.8497206] [1.0741079]\n",
      "2. 0.7524663209915161 [1.9572839] [1.0891565]\n",
      "3. 0.03630606085062027 [1.9803681] [1.0920721]\n",
      "4. 0.0034325658343732357 [1.9853568] [1.0923902]\n",
      "5. 0.0019094638992100954 [1.9864691] [1.0921531]\n",
      "6. 0.0018249232089146972 [1.9867511] [1.0917984]\n",
      "7. 0.0018064674222841859 [1.9868549] [1.0914198]\n",
      "8. 0.0017911831382662058 [1.9869205] [1.0910374]\n",
      "9. 0.0017761692870408297 [1.9869776] [1.0906554]\n",
      "10. 0.0017612905940040946 [1.9870328] [1.0902748]\n",
      "11. 0.0017465248238295317 [1.9870874] [1.0898957]\n",
      "12. 0.0017318788450211287 [1.9871415] [1.0895182]\n",
      "13. 0.001717371167615056 [1.9871956] [1.0891423]\n",
      "14. 0.0017029836308211088 [1.9872494] [1.088768]\n",
      "15. 0.001688705524429679 [1.9873029] [1.0883952]\n",
      "16. 0.0016745462780818343 [1.9873562] [1.088024]\n",
      "17. 0.0016605176497250795 [1.9874092] [1.0876544]\n",
      "18. 0.0016466060187667608 [1.9874622] [1.0872862]\n",
      "19. 0.0016327971825376153 [1.9875147] [1.0869197]\n",
      "20. 0.001619120710529387 [1.9875672] [1.0865546]\n",
      "21. 0.0016055454034358263 [1.9876194] [1.0861912]\n",
      "22. 0.0015920869773253798 [1.9876714] [1.0858293]\n",
      "23. 0.00157875195145607 [1.9877232] [1.0854688]\n",
      "24. 0.0015655119204893708 [1.9877748] [1.0851098]\n",
      "25. 0.001552393427118659 [1.9878261] [1.0847524]\n",
      "26. 0.001539376680739224 [1.9878772] [1.0843965]\n",
      "27. 0.0015264693647623062 [1.9879282] [1.0840421]\n",
      "28. 0.0015136749716475606 [1.9879788] [1.0836891]\n",
      "29. 0.0015009951312094927 [1.9880294] [1.0833377]\n",
      "30. 0.0014884199481457472 [1.9880797] [1.0829877]\n",
      "31. 0.0014759391779080033 [1.9881297] [1.0826391]\n",
      "32. 0.0014635641127824783 [1.9881796] [1.0822921]\n",
      "33. 0.0014512985944747925 [1.9882292] [1.0819465]\n",
      "34. 0.0014391348231583834 [1.9882786] [1.0816023]\n",
      "35. 0.0014270752435550094 [1.9883279] [1.0812596]\n",
      "36. 0.0014151122886687517 [1.9883769] [1.0809183]\n",
      "37. 0.001403255620971322 [1.9884256] [1.0805784]\n",
      "38. 0.0013914851006120443 [1.9884742] [1.08024]\n",
      "39. 0.001379816560074687 [1.9885226] [1.079903]\n",
      "40. 0.0013682548888027668 [1.9885708] [1.0795674]\n",
      "41. 0.0013567933347076178 [1.9886189] [1.0792333]\n",
      "42. 0.0013454158324748278 [1.9886667] [1.0789006]\n",
      "43. 0.0013341347221285105 [1.9887142] [1.0785692]\n",
      "44. 0.0013229565229266882 [1.9887617] [1.0782392]\n",
      "45. 0.0013118610950186849 [1.9888089] [1.0779107]\n",
      "46. 0.001300874981097877 [1.9888558] [1.0775834]\n",
      "47. 0.001289975130930543 [1.9889027] [1.0772576]\n",
      "48. 0.0012791624758392572 [1.9889493] [1.0769331]\n",
      "49. 0.001268440973944962 [1.9889957] [1.0766101]\n",
      "50. 0.0012578030582517385 [1.9890419] [1.0762883]\n",
      "51. 0.0012472665403038263 [1.9890879] [1.0759679]\n",
      "52. 0.0012368109310045838 [1.9891338] [1.0756489]\n",
      "53. 0.0012264398392289877 [1.9891794] [1.0753312]\n",
      "54. 0.001216154545545578 [1.9892248] [1.0750148]\n",
      "55. 0.0012059661094099283 [1.9892701] [1.0746998]\n",
      "56. 0.0011958589311689138 [1.9893152] [1.074386]\n",
      "57. 0.0011858276557177305 [1.98936] [1.0740736]\n",
      "58. 0.0011758911423385143 [1.9894047] [1.0737625]\n",
      "59. 0.0011660383315756917 [1.9894491] [1.0734528]\n",
      "60. 0.0011562684085220098 [1.9894935] [1.0731443]\n",
      "61. 0.0011465734569355845 [1.9895376] [1.0728371]\n",
      "62. 0.0011369629064574838 [1.9895816] [1.0725312]\n",
      "63. 0.0011274277931079268 [1.9896253] [1.0722266]\n",
      "64. 0.0011179839493706822 [1.9896688] [1.0719234]\n",
      "65. 0.0011086123995482922 [1.9897122] [1.0716213]\n",
      "66. 0.0010993295582011342 [1.9897555] [1.0713205]\n",
      "67. 0.001090114819817245 [1.9897985] [1.0710211]\n",
      "68. 0.0010809708619490266 [1.9898413] [1.0707228]\n",
      "69. 0.0010719195706769824 [1.989884] [1.0704259]\n",
      "70. 0.001062933006323874 [1.9899265] [1.0701301]\n",
      "71. 0.001054022111929953 [1.9899688] [1.0698355]\n",
      "72. 0.0010451815323904157 [1.9900109] [1.0695423]\n",
      "73. 0.0010364328045397997 [1.9900528] [1.0692502]\n",
      "74. 0.0010277455439791083 [1.9900947] [1.0689594]\n",
      "75. 0.0010191192850470543 [1.9901363] [1.0686698]\n",
      "76. 0.0010105832479894161 [1.9901776] [1.0683814]\n",
      "77. 0.001002120552584529 [1.9902189] [1.0680943]\n",
      "78. 0.0009937100112438202 [1.99026] [1.0678083]\n",
      "79. 0.0009853870142251253 [1.9903009] [1.0675235]\n",
      "80. 0.0009771209442988038 [1.9903417] [1.0672399]\n",
      "81. 0.0009689411381259561 [1.9903822] [1.0669575]\n",
      "82. 0.0009608101099729538 [1.9904226] [1.0666763]\n",
      "83. 0.0009527590009383857 [1.9904628] [1.0663962]\n",
      "84. 0.000944768835324794 [1.9905028] [1.0661174]\n",
      "85. 0.0009368503233417869 [1.9905428] [1.0658398]\n",
      "86. 0.0009290032321587205 [1.9905825] [1.0655633]\n",
      "87. 0.0009212227305397391 [1.990622] [1.065288]\n",
      "88. 0.0009135023574344814 [1.9906614] [1.0650138]\n",
      "89. 0.0009058352443389595 [1.9907006] [1.0647408]\n",
      "90. 0.0008982503786683083 [1.9907397] [1.0644689]\n",
      "91. 0.0008907241863198578 [1.9907786] [1.0641981]\n",
      "92. 0.000883255444932729 [1.9908173] [1.0639285]\n",
      "93. 0.0008758519543334842 [1.9908558] [1.06366]\n",
      "94. 0.0008685143548063934 [1.9908942] [1.0633926]\n",
      "95. 0.0008612339152023196 [1.9909325] [1.0631264]\n",
      "96. 0.0008540117414668202 [1.9909706] [1.0628613]\n",
      "97. 0.0008468530140817165 [1.9910085] [1.0625973]\n",
      "98. 0.0008397485362365842 [1.9910463] [1.0623344]\n",
      "99. 0.0008327148971147835 [1.9910839] [1.0620726]\n"
     ]
    }
   ],
   "source": [
    "for step in range(100):\n",
    "    cost = train_step()\n",
    "    print(f'{step}. {cost.numpy()} {W.numpy()} {b.numpy()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습에 사용된 데이터가 아닌 랜덤한 데이터로 예측해본다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:     [197. 161. 129. 119.  45.]\n",
      "Prediction: [196.1883   160.34879  128.49144  118.53602   44.865917]\n"
     ]
    }
   ],
   "source": [
    "x_test = np.random.randint(1, 101, 5).astype(np.float32)\n",
    "y_test = x_test * 2 + 1\n",
    "\n",
    "print('Target:    ', y_test)\n",
    "print('Prediction:', hypothesis(x_test).numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}